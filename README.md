# Linear Regression with OLS, SVD, Gradient Descent and PCA

This project implements and compares several approaches to solving linear regression, using a real-world dataset from `sklearn.datasets`.  
All core methods are implemented manually in NumPy, without using `sklearn`'s regression models, to emphasize understanding of the underlying linear algebra and optimization.


## 1. Project Overview

**Main goals:**

- Formulate linear regression as a least-squares problem.
- Solve it using:
  - Ordinary Least Squares (OLS) via the normal equation
  - SVD-based pseudoinverse
  - Batch Gradient Descent
- Apply **PCA** for dimensionality reduction and rerun regression in the reduced space.
- Compare methods in terms of:
  - Train and test error (MSE)
  - Numerical stability (ill-conditioning)
  - Convergence behaviour
  - Effect of dimensionality on performance

**Dataset:**

- `sklearn.datasets.load_diabetes`
- Regression task: predict a continuous measure of diabetes disease progression.
- 442 samples, 10 numerical features (age, BMI, blood pressure, etc.).

All data is loaded directly from `sklearn`; no external files are required.



## 2. Repository Structure

```text
.
├── project.ipynb          # Main notebook with full implementation and results
├── README.md              # This file
├── requirements.txt       # Python dependencies for the project
├── figures/               # Output plots generated by project.ipynb (created at runtime)
├── report/
│   └── report.pdf         # Final written report (to be added by student)
└── README.md

> **Note:** The core of the project (for grading) is in **`project.ipynb`**, which contains all code, explanations, and numerical results.



## 3. Environment and Dependencies

This project was developed and tested with:

* **Python:** 3.10.10
* **Required packages:**

  * `numpy`
  * `matplotlib`
  * `pandas`
  * `scikit-learn`
  * `jupyter` or `jupyterlab`

### 3.1. Setting up a Virtual Environment (recommended)

From the project root:

**Windows (PowerShell):**

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```

**Linux/macOS (bash):**

```bash
python -m venv .venv
source .venv/bin/activate
```

### 3.2. Installing Dependencies

Using the provided `requirements.txt`:

```bash
pip install -r requirements.txt
```

Or manually:

```bash
pip install numpy matplotlib pandas scikit-learn jupyter
```



## 4. How to Run the Notebook

1. Activate the virtual environment (see above).

2. Start Jupyter:

   ```bash
   jupyter notebook
   ```

   or

   ```bash
   jupyter lab
   ```

3. Open **`project.ipynb`**.

4. Run all cells **from top to bottom** (`Kernel → Restart & Run All`) to:

   * Load and preprocess the dataset,
   * Fit models using OLS, SVD, and Gradient Descent,
   * Perform PCA and regression in reduced dimensions,
   * Generate all plots and quantitative results.

The notebook is designed to be **fully reproducible**: running it end-to-end should regenerate all results used in the report.



## 5. Implementation Summary (by Task)

### Task 1 – Dataset Preparation

* Load the diabetes dataset via `load_diabetes()`.
* Split into train/test sets (`train_test_split`).
* Standardize features using **training-set mean and std only**.
* Add a **bias column** to form the design matrices (X_{\text{train}}) and (X_{\text{test}}).

### Task 2 – Ordinary Least Squares (Normal Equation)

* Implement:
  [
  \hat{\beta} = (X^\top X)^{-1} X^\top y
  ]
* Compute and report **train/test MSE**.
* Plot residuals vs. predicted values.
* Discuss limitations of the normal equation (singular and ill-conditioned (X^\top X)).

### Task 3 – SVD-Based Solution

* Compute (X = U \Sigma V^\top) and the pseudoinverse (X^+ = V \Sigma^+ U^\top).
* Solve:
  [
  \hat{\beta} = X^+ y
  ]
* Compare coefficients and MSE with the OLS solution.
* Plot singular values of the design matrix.
* Demonstrate robustness on an **ill-conditioned** version of (X) (with near-duplicate features).

### Task 4 – Gradient Descent

* Define MSE loss:
  [
  L(\beta) = \frac{1}{2n} |X \beta - y|^2
  ]
* Implement batch Gradient Descent with:
  [
  \nabla_\beta L(\beta) = \frac{1}{n} X^\top (X \beta - y)
  ]
* Experiment with different learning rates.
* Plot **loss vs. iterations** for each learning rate.
* Compare the converged GD solution with the analytical OLS/SVD solution (β and MSE).

### Task 5 – PCA and Dimensionality Reduction

* Use SVD on the standardized training data to obtain principal components.
* Compute **explained variance ratio** and **cumulative variance**.
* Project data onto the top-k PCs and run regression for each k.
* Plot **MSE vs. k** and compare with full-dimensional OLS.
* Discuss the trade-off between dimensionality, variance explained, and model performance.



## 6. Reproducibility and Evaluation

* All random operations (e.g., train/test split) use a fixed `random_state` / `seed` for reproducibility.
* The notebook computes:

  * Train/Test MSE for each method,
  * Norm differences between solutions (e.g., OLS vs SVD, OLS vs GD),
  * PCA-based performance across different numbers of components.
* These values can be reported directly in the final PDF report.



## 7. Notes for the Instructor

* The notebook focuses on **from-scratch implementations** (NumPy) to highlight understanding of:

  * Linear algebra formulations (normal equation, SVD),
  * Numerical stability and ill-conditioning,
  * Optimization via gradient descent,
  * PCA as a dimensionality reduction tool prior to regression.
* No external data files are used; the entire experiment is self-contained and reproducible by simply running `project.ipynb` after installing the listed dependencies.



## 8. About the `figures/` Folder

The `figures/` directory is used to store plots generated by the notebook, such as:

* `ols_residuals_train.png`
* `ols_univariate_bmi.png`
* `svd_singular_values.png`
* `gd_loss_curves.png`
* `pca_explained_variance.png`
* `pca_cumulative_variance.png`
* `pca_error_vs_k.png`
* `pca_2d_scatter.png` (if the optional cell is executed)

If the folder is currently **empty**, that is expected **before** running the notebook.

* When you run all cells in `project.ipynb`, the figures will be automatically saved into `figures/`.
* For submission, you can:

  * **Option A (recommended):** Run the notebook once, let it generate all figures, and include the populated `figures/` folder in your ZIP/submission. These images can also be reused in your `report.pdf`.
  * **Option B:** If your instructor only grades the notebook and report, you can still keep the empty `figures/` folder in the project so paths remain valid when they execute the notebook.

Prepared By,

Aaish Faisal Hameedi (25k-7608)
Mehdi Abbas (25k-7601)